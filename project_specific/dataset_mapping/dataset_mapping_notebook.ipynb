{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import io\n",
    "import re\n",
    "import json\n",
    "from urllib.parse import urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "import imgkit\n",
    "import openpyxl as px\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "\n",
    "def parse_domain(url):\n",
    "    domain = urlparse(url).netloc.replace('www.', '')\n",
    "    return domain\n",
    "\n",
    "def request_csv(url):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    return pd.read_csv(io.StringIO(response.text)).to_dict(orient='records')\n",
    "\n",
    "def get_CISA_MAP():\n",
    "    if os.path.exists('CISA_MAP.json'):\n",
    "        with open('CISA_MAP.json', 'r') as f:\n",
    "            CISA_MAP = json.load(f)\n",
    "    else:\n",
    "        CISA_MAP = {}\n",
    "        CISA = request_csv('https://github.com/cisagov/dotgov-data/blob/main/current-federal.csv?raw=true')\n",
    "        for row in CISA:\n",
    "            CISA_MAP[row['Domain name']] = row\n",
    "        with open('CISA_MAP.json', 'w') as f:\n",
    "            json.dump(CISA_MAP, f)\n",
    "    return CISA_MAP\n",
    "\n",
    "def cut_text_to_n_tokens(text, n=500):\n",
    "    tokens = text.split()\n",
    "    if len(tokens) > n:\n",
    "        text = ' '.join(tokens[:n])\n",
    "    return text\n",
    "\n",
    "def get_soup(url):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    return BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "def short_soup(soup):\n",
    "    for tag in soup.find_all(['script', 'style']):\n",
    "        tag.decompose()\n",
    "    return soup\n",
    "\n",
    "def screenshot(url):\n",
    "    image_location = f'{url.split(\"/\")[-1]}_screenshot.jpg'\n",
    "    imgkit.from_url(url, image_location)\n",
    "    return image_location\n",
    "\n",
    "\n",
    "def whosAgency(url, temperature=0.2):\n",
    "    CISA_MAP = get_CISA_MAP()\n",
    "    domain = parse_domain(url)\n",
    "    if domain in CISA_MAP:\n",
    "        agency_response = CISA_MAP[domain]\n",
    "    else:\n",
    "        soup = get_soup(url)\n",
    "        links = [{x['href']: x.text} for x in soup.find_all('a') if x.get('href') and x.get('href').startswith('http')]\n",
    "        text = soup.get_text()\n",
    "        link_json = json.dumps(links)\n",
    "        token_cut = 1500\n",
    "        text = cut_text_to_n_tokens(text, token_cut - len(link_json.split()))\n",
    "        website_info = {url: {'text': text, 'links': links}}\n",
    "\n",
    "        messages = [\n",
    "            SystemMessage(content=\"\"\"You are an AI assistant trained to analyze website content and determine the US federal agency that owns or is responsible for the website. You should focus on textual content, official logos, disclaimers, contact information, and any relevant metadata. All responses must be in this json format:  \n",
    "                            {\n",
    "                                'url': {dhs sub-bureau non-dot-gov url},\n",
    "                                'Domain name': 'dhs.gov',\n",
    "                                'Domain type': 'Federal - Executive',\n",
    "                                'Agency': 'Department of Homeland Security',\n",
    "                                'Organization name': 'Headquarters',\n",
    "                                'City': 'Washington',\n",
    "                                'State': 'DC',\n",
    "                                'Security contact email': 'IS2OSecurity@hq.dhs.gov'\n",
    "                            }\n",
    "                                we want to union this information with the CISA MAP. If you can't determine the agency, please don't guess. simply fill in the field as \"NONE FOUND\" in the JSON response.\"\"\"),\n",
    "            HumanMessage(content=f\"Please fill out the metadata json if you can determine the agency that is responsible for {website_info}? Please fill out the requested metadata json for this url. Also - make sure to use formal names, never abbreviations, nicknames, or monikers.\")\n",
    "        ]\n",
    "        \n",
    "        llm = ChatOpenAI(api_key=api_key, model=\"gpt-3.5-turbo\", temperature=temperature)\n",
    "        agency_response = llm(messages)\n",
    "    messages = [\n",
    "        SystemMessage(content=\"\"\"You are an AI assistant trained to analyze website content and determine which US Program Office collected or published the data. You should focus on textual content, official logos, disclaimers, contact information, and any relevant metadata. All responses must be in this json format:  \n",
    "                        {\n",
    "                            'url': 'https://github.com/cisagov',\n",
    "                            'program name': 'Cybersecurity and Infrastructure Security Agency (CISA)',\n",
    "                            'program main url': 'https://www.cisa.gov/',\n",
    "                            'Domain name': 'dhs.gov',\n",
    "                            'Domain type': 'Federal - Executive',\n",
    "                            'Agency': 'Department of Homeland Security',\n",
    "                            'Organization name': 'Headquarters',\n",
    "                            'City': 'Washington',\n",
    "                            'State': 'DC',\n",
    "                            'Security contact email': 'IS2OSecurity@hq.dhs.gov'\n",
    "                        }\n",
    "                            we want to union this information with the Federal Budget Database. If you can't determine the specific program office, look for the most prominent text on the page taht loooks like a title and use a derivation of that with a (prelim) note. please don't guess without providing evidence that the results are imperfect\".\"\"\"),\n",
    "        HumanMessage(content=f\"Please fill out the metadata json if you can determine the program office that is responsible for {website_info}? Please fill out the requested metadata json for this url. Also - make sure to use formal names, never abbreviations, nicknames, or monikers.\")\n",
    "    ]\n",
    "    \n",
    "    llm = ChatOpenAI(api_key=api_key, model=\"gpt-3.5-turbo\", temperature=temperature)\n",
    "    source_response = llm(messages)\n",
    "    \n",
    "    return agency_response, source_response\n",
    "\n",
    "def auto_sourcery(url):\n",
    "    agency_response, source_response = whosAgency(url, temperature=0.5)\n",
    "    agency = json.loads(agency_response.content.replace(\"'\", '\"'))\n",
    "    source = {} if source_response is None else json.loads(source_response.content.replace(\"'\", '\"'))\n",
    "\n",
    "    agency['agency_display_name'] = agency['Agency']\n",
    "    agency['source_display_name'] = source.get('program name', '')\n",
    "    agency['source_display_url'] = source.get('program main url', '')\n",
    "    agency['agency_display_url'] = f'https://www.{agency[\"Domain name\"]}'\n",
    "    return agency\n",
    "\n",
    "def get_data_to_map(data_path):\n",
    "    data = px.load_workbook(data_path, read_only=True)\n",
    "    data_dict = {}\n",
    "    for sheet in data.sheetnames:\n",
    "        data_dict[sheet] = pd.DataFrame(data[sheet].values)\n",
    "    return data_dict\n",
    "\n",
    "def get_data_path_map(data_path):\n",
    "    # fact_results = []\n",
    "    df = get_data_to_map(data_path)['result']\n",
    "    df.columns = df.iloc[0]\n",
    "    df = df.drop(0)\n",
    "    return df\n",
    "\n",
    "def categorize_column(df,series):\n",
    "    # check to see if it has time. \n",
    "    if any([x in series.lower() for x in time_headers]):\n",
    "        return 'TIME__'\n",
    "    # check to see if it has location\n",
    "    if any([x in series.lower() for x in location_headers]):\n",
    "        return 'LOC__'\n",
    "    # check to see if it has value\n",
    "    if any([x in series.lower() for x in standard_facts]):\n",
    "        return 'FACT__'\n",
    "    if any([x in series.lower() for x in standard_dims]):\n",
    "        return 'DIM__'\n",
    "    if len(df[series]) < (len(df[series].unique()) * 3):\n",
    "        return 'FACT__'\n",
    "    return 'DIM__'\n",
    "\n",
    "\n",
    "def publish_dimension(prefix, series_name, unique_value):\n",
    "    if unique_value:\n",
    "        return {'raw_column': series_name, 'transformed_column': f'{prefix}{series_name}'} #, 'raw_value': unique_value, 'transformed_value': unique_value}\n",
    "    return {'raw_column': series_name, 'transformed_column': f'{prefix}{series_name}'}\n",
    "\n",
    "def publish_fact(series):\n",
    "    return {'fact_column_name': series.name, 'fact_display_name': f'{prefix}{replace_special_chacters_and_spaces(series.name)}'}\n",
    "def publish_table(table_lookups):\n",
    "    table_lookups = {k: v for k, v in table_lookups.items() if k in ['source_display_name', 'source_display_url', 'agency_display_name', 'agency_display_url']}\n",
    "    table_lookups = pd.DataFrame(table_lookups, index=[0]).drop_duplicates()\n",
    "    return table_lookups\n",
    "def replace_special_chacters_and_spaces(string):\n",
    "    return re.sub(r'\\W+', '_', string)\n",
    "\n",
    "def dfs_to_excel(dfs_dict, data_path):\n",
    "    \"\"\"\n",
    "    Converts a dictionary of pandas DataFrames to an Excel file with multiple sheets.\n",
    "\n",
    "    Parameters:\n",
    "    dfs_dict (dict): A dictionary where the keys are sheet names and the values are DataFrames.\n",
    "    file_path (str): The path to save the Excel file.\n",
    "    \"\"\"\n",
    "    with pd.ExcelWriter(data_path, engine='xlsxwriter') as writer:\n",
    "        for sheet_name, df in dfs_dict.items():\n",
    "            df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "        writer.close()  # Add this line to save the Excel file\n",
    "\n",
    "        \n",
    "# Define patterns for identifying date/year columns and location columns\n",
    "date_patterns = [r'^\\d{4}$', r'\\d{4}-\\d{2}-\\d{2}', r'\\d{2}/\\d{2}/\\d{4}', r'\\d{2}-\\d{2}-\\d{4}']\n",
    "location_keywords = ['state', 'united states', 'county', 'metro', 'city', 'town', 'location', 'region', 'geographic']\n",
    "number_pattern = re.compile(r'^\\d+(\\.\\d+)?$')\n",
    "\n",
    "def what_is_being_measured(context, url, temperature=0.2, soup=None):\n",
    "    if not soup:\n",
    "        soup = get_soup(url)\n",
    "    text = soup.get_text()\n",
    "    website_info = cut_text_to_n_tokens(text, 1000)\n",
    "\n",
    "    messages = [\n",
    "        SystemMessage(content=\"\"\"You are an AI assistant trained to analyze website content and determine what is being measured, the unit of measurement, the field type, and a easily understood display name.\n",
    "Let me give you some examples: \n",
    "1. \"The number of people in the United States is 330 million\", you would respond with: {\"fact_display_name\": \"number of people\", \"units\": \"people\", \"unit_display_name\": \"people\", \"unit_type\": \"INT\"},\n",
    "2. \"'Department of State\\nBureau of Population, Refugees, and Migration\\nOffice of Admissions - Refugee Processing Center\\nSummary of Refugee Admissions. as of 30-April-2024. Cumulative Summary of Refugee Admissions', you would respond with {\"fact_display_name\": \"Refugee Admissions\", \"units\": \"people\", \"unit_display_name\": \"people\", \"unit_type\": \"INT\"}.\n",
    "\n",
    "Please respond in this JSON format:\n",
    "{\n",
    "    \"fact_display_name\": \"measurement\",\n",
    "    \"units\": \"unit of measure\",\n",
    "    \"unit_display_name\": \"unit to display\", # typically the same as unit\n",
    "    \"unit_type\": \"unit_type\" # may be INT, FLOAT, or STRING\n",
    "}\"\"\"),\n",
    "        HumanMessage(content=f\"Please review this information from the data file: {context} and this website information: {website_info}. Please fill out the requested metadata JSON for this URL.\")\n",
    "    ]\n",
    "    \n",
    "    llm = ChatOpenAI(api_key=api_key, model=\"gpt-3.5-turbo\", temperature=temperature)\n",
    "    fact_response = llm(messages)\n",
    "    return json.loads(fact_response.content) if json.loads(fact_response.content) else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = 'sk-proj-pGa1oIafi3C89siUTdluT3BlbkFJbYpCeKCwrnLomY2zF96M'\n",
    "url = 'https://www.wrapsnet.org/admissions-and-arrivals/'\n",
    "data_path = '/Users/robertstillwell/robcode/mapping_work/Beta/RPC_Arrivals_Admissions.xlsx'\n",
    "exclude_date_keywords = ['authorized', 'source']\n",
    "forced_dimension_columns = ['table', 'source','header']\n",
    "forced_fact_columns = ['Value']\n",
    "\n",
    "url_soup = get_soup(url)\n",
    "api_key = os.getenv('OPENAI_API_KEY') if os.getenv('OPENAI_API_KEY') else OPENAI_API_KEY\n",
    "table_lookups = auto_sourcery(url)\n",
    "mapped_data = []\n",
    "time_headers = ['date', 'time', 'year', 'month', 'week', 'day','quarter']\n",
    "location_headers = ['location', 'city', 'state', 'country', 'metro', 'country','nation','region','division']\n",
    "standard_dims = []\n",
    "standard_facts = ['Value']\n",
    "dataset_mapping = []\n",
    "fact_lookups = []\n",
    "data_df = get_data_path_map(data_path)\n",
    "ignore_location = True\n",
    "\n",
    "for col in data_df.columns:\n",
    "    series = data_df[col]\n",
    "    prefix = categorize_column(data_df, col)\n",
    "    if prefix == 'LOC__' and ignore_location:\n",
    "        prefix = 'DIM__'\n",
    "    if prefix in ('DIM__', 'LOC__', 'TIME'):\n",
    "        uniques = series.unique()\n",
    "        for unique in uniques:\n",
    "            if pd.isna(unique) or unique.strip() == '':\n",
    "                continue\n",
    "            dimension = publish_dimension(prefix, series.name, unique)\n",
    "            dataset_mapping.append(dimension)\n",
    "    elif prefix == 'FACT__':\n",
    "        context = data_df['Header'][1]\n",
    "        fact_metadata = what_is_being_measured(context, url, temparature=0.2, soup=url_soup)\n",
    "        fact_metadata['fact_column_name'] = series.name\n",
    "        fact_metadata['fact_display_name'] = f\"{prefix}{replace_special_chacters_and_spaces(fact_metadata['fact_display_name'])}\"\n",
    "        fact_metadata['transformed_column']: f'{prefix}{series.name}'\n",
    "        fact_lookups.append(fact_metadata)\n",
    "        dim_metadata = {'raw_column': series.name, 'transformed_column': fact_metadata['fact_display_name']}\n",
    "        dataset_mapping.append(dim_metadata)\n",
    "\n",
    "        # dataset_mapping.append([fact_metadata])\n",
    "table_lookups = pd.DataFrame(table_lookups, index=[0])[['agency_display_name','agency_display_url','source_display_name','source_display_url']]\n",
    "\n",
    "fact_lookups = pd.DataFrame(fact_lookups)\n",
    "fact_lookups_columns = [x.strip() for x in 'fact_column_name\tfact_display_name\tunit\tunit_display_name\tunit_type\tadjustments\tadjustments_display_name\tglossary_text'.split('\\t')]\n",
    "for column in fact_lookups_columns:\n",
    "    if column not in fact_lookups.columns:\n",
    "        fact_lookups[column] = None\n",
    "        \n",
    "\n",
    "\n",
    "dataset_mapping = pd.DataFrame(dataset_mapping)\n",
    "dataset_mapping_columns = [x.strip() for x in 'acquisition_url\tstart_date\tend_date\traw_column\traw_value\ttransformed_column\ttransformed_value\tcolumn_display_name\tvalue_display_name\tgold_table_name'.split('\\t')]\n",
    "for column in dataset_mapping_columns:\n",
    "    if column not in dataset_mapping.columns:\n",
    "        if column == 'acquisition_url':\n",
    "            dataset_mapping[column] = data_path.split('/')[-1]\n",
    "        else: \n",
    "            dataset_mapping[column] = None\n",
    "dataset_mapping = dataset_mapping[dataset_mapping_columns].drop_duplicates()\n",
    "results_dict = {'dataset_mapping':dataset_mapping, 'table_lookups':table_lookups, 'fact_lookups':fact_lookups}\n",
    "\n",
    "dfs_to_excel(results_dict, data_path.replace('.xlsx','_mapping.xlsx'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
